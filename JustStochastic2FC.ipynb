{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "YUgYCHQtYW5z"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.cuda as cuda\n",
        "# Load in relevant libraries, and alias where appropriate\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import tensorflow as tf\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Parameter\n",
        "from torch.nn.modules.conv import _ConvNd, init\n",
        "from torch.nn.modules.utils import _single, _pair, _triple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "N_EOwyDV-6mw"
      },
      "outputs": [],
      "source": [
        "def zeropoint_quantize(X,W):\n",
        "\n",
        "    tensor_input = torch.tensor(X)\n",
        "    tensor_weight = torch.tensor(W)\n",
        "    #getting Mins of each array\n",
        "    Mins=torch.empty(2)\n",
        "    #print(\"Mins=\",Mins)\n",
        "    Mins[0]=torch.tensor(torch.min(tensor_input))\n",
        "    Mins[1]=torch.tensor(torch.min(tensor_weight))\n",
        "\n",
        "    #print(\"Mins=\",Mins)\n",
        "\n",
        "    #getting Maxs of each array\n",
        "    Maxs=torch.empty(2)\n",
        "    #print(\"Maxs=\",Maxs)\n",
        "    Maxs[0]=torch.tensor(torch.max(tensor_input))\n",
        "    Maxs[1]=torch.tensor(torch.max(tensor_weight))\n",
        "\n",
        "    #print(\"Maxs=\",Maxs)\n",
        "\n",
        "\n",
        "    # Calculate value range (denominator)\n",
        "    x_range = torch.max(Maxs) - torch.min(Mins)\n",
        "    #print(\"x_range=\",x_range)\n",
        "    x_range = 1 if x_range == 0 else x_range\n",
        "\n",
        "    # Calculate scale\n",
        "    scale = 255 / x_range\n",
        "    #print(\"scale=\",scale)\n",
        "\n",
        "\n",
        "    # Shift by zero-point\n",
        "    zeropoint = (-scale * torch.min(Mins)).round()\n",
        "\n",
        "\n",
        "    # Scale and round the inputs\n",
        "    X_quant = torch.clip((X * scale + zeropoint).round(), 0, 255)\n",
        "    W_quant = torch.clip((W * scale + zeropoint).round(), 0, 255)\n",
        "\n",
        "\n",
        "    #print(\"X_quant=\",X_quant)\n",
        "    #print(\"X_quant=\",W_quant)\n",
        "\n",
        "    return X_quant.to(torch.int32),W_quant.to(torch.int32),scale,zeropoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "D1tX_k42Kpbq"
      },
      "outputs": [],
      "source": [
        "def zeropoint_dequantize(X,z,scale,sumation):\n",
        "\n",
        "    # Dequantize\n",
        "    X_dequant = (X - (z*z)-(scale*z*sumation)) / (scale*scale)\n",
        "\n",
        "    #print(\"X_dequant=\",X_dequant)\n",
        "\n",
        "\n",
        "    return  X_dequant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "Qm3Zl2fjB-cq"
      },
      "outputs": [],
      "source": [
        "# Define relevant variables for the ML task\n",
        "batch_size = 64\n",
        "num_classes = 10\n",
        "learning_rate = 0.001\n",
        "num_epochs = 4\n",
        "\n",
        "# Device will determine whether to run the training on GPU or CPU.\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "#Loading the dataset and preprocessing\n",
        "train_dataset = torchvision.datasets.MNIST(root = './data',\n",
        "                                           train = True,\n",
        "                                           transform = transforms.Compose([\n",
        "                                                  transforms.Resize((32,32)),\n",
        "                                                  transforms.ToTensor(),\n",
        "                                                  transforms.Normalize(mean = (0.1307,), std = (0.3081,))]),\n",
        "                                           download = True)\n",
        "\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(root = './data',\n",
        "                                          train = False,\n",
        "                                          transform = transforms.Compose([\n",
        "                                                  transforms.Resize((32,32)),\n",
        "                                                  transforms.ToTensor(),\n",
        "                                                  transforms.Normalize(mean = (0.1325,), std = (0.3105,))]),\n",
        "                                          download=True)\n",
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
        "                                           batch_size = batch_size,\n",
        "                                           shuffle = True)\n",
        "\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
        "                                           batch_size = batch_size,\n",
        "                                           shuffle = True)\n",
        "\n",
        "\n",
        "class nnModel(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(nnModel, self).__init__()\n",
        "\n",
        "        self.conv1 =  nn.Conv2d(1, 6, kernel_size=5)\n",
        "        self.relu1=nn.ReLU()\n",
        "        self.maxpool1=nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
        "        self.relu2=nn.ReLU()\n",
        "        self.maxpool2=nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
        "        self.conv3 = nn.Conv2d(16, 120, kernel_size=5)\n",
        "\n",
        "        self.fc1 = nn.Linear(120, 84)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(84, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.relu1(out)\n",
        "        out = self.maxpool1(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.relu2(out)\n",
        "        out = self.maxpool2(out)\n",
        "        out = self.conv3(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.fc1(out)\n",
        "        out = self.relu4(out)\n",
        "        out = self.fc2(out)\n",
        "\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "mKWP69Wwnzj1"
      },
      "outputs": [],
      "source": [
        "# Load the saved model weights\n",
        "model = nnModel(num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BojUUCsAu9sh",
        "outputId": "9a89cb46-2e78-4c55-a41e-fa80fd786afe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'conv3.weight', 'conv3.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias'])\n"
          ]
        }
      ],
      "source": [
        "print(model.state_dict().keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEOm7r4Tn92t",
        "outputId": "f2656563-48b8-48ef-e0ea-c2d0dd813893"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ],
      "source": [
        "model.load_state_dict(torch.load('/content/drive/MyDrive/lenet-weight/weightsLenetPytorch3Conv-2.pt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "jW2X4jeXNcHa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ce00321-e57f-428a-9e87-276d59b0f984"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "layer_weightsfc1= torch.Size([84, 120])\n",
            "layer_biasfc1= torch.Size([84])\n",
            "layer_weightsfc2= torch.Size([10, 84])\n",
            "layer_biasfc2= torch.Size([10])\n"
          ]
        }
      ],
      "source": [
        "layer_weights1 = model.state_dict()['conv1.weight']\n",
        "layer_bias1 = model.state_dict()['conv1.bias']\n",
        "layer_weights2 = model.state_dict()['conv2.weight']\n",
        "layer_bias2 = model.state_dict()['conv2.bias']\n",
        "layer_weights3 = model.state_dict()['conv3.weight']\n",
        "layer_bias3 = model.state_dict()['conv3.bias']\n",
        "layer_weightsfc1 = model.state_dict()['fc1.weight']\n",
        "layer_biasfc1 = model.state_dict()['fc1.bias']\n",
        "layer_weightsfc2 = model.state_dict()['fc2.weight']\n",
        "layer_biasfc2 = model.state_dict()['fc2.bias']\n",
        "#print(\"layer_weights1=\",layer_weights1)\n",
        "#print(\"layer_bias1=\",layer_bias1)\n",
        "#print(\"layer_weights2=\",layer_weights2)\n",
        "#print(\"layer_bias2=\",layer_bias2)\n",
        "#print(\"layer_weights3=\",layer_weights3)\n",
        "#print(\"layer_bias3=\",layer_bias3)\n",
        "print(\"layer_weightsfc1=\",layer_weightsfc1.shape)\n",
        "print(\"layer_biasfc1=\",layer_biasfc1.shape)\n",
        "print(\"layer_weightsfc2=\",layer_weightsfc2.shape)\n",
        "print(\"layer_biasfc2=\",layer_biasfc2.shape)\n",
        "#print(layer_weights.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layer_weights1.shape,layer_weights2.shape,layer_weights3.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0YWgCmxN_r5",
        "outputId": "cfe06260-d72e-4098-ec67-59b01d504721"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([6, 1, 5, 5]),\n",
              " torch.Size([16, 6, 5, 5]),\n",
              " torch.Size([120, 16, 5, 5]))"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FCLayer1(nn.Linear):\n",
        "    def __init__(self, n_inputs, n_outputs):\n",
        "        super(FCLayer1, self).__init__(n_inputs, n_outputs)\n",
        "        print(\"n_inputsfc1=\",n_inputs)\n",
        "        print(\"n_outputsfc1=\",n_outputs)\n",
        "        self.n_inputs = n_inputs\n",
        "        self.n_outputs = n_outputs\n",
        "\n",
        "        # Initialize weights and biases as PyTorch tensors\n",
        "        self.weights = layer_weightsfc1#nn.Parameter(torch.randn(n_outputs, n_inputs))\n",
        "        self.biases =layer_biasfc1# nn.Parameter(torch.zeros(n_outputs))\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        #////////////////////////////Skippy Multiplicant///////////////////////////////////\n",
        "        XBit=8\n",
        "\n",
        "        def UpDownCounter(Enable,x,IncDec):\n",
        "          IntX=int(x)\n",
        "\n",
        "          if Enable != 0:\n",
        "            if IntX == 1:\n",
        "              if IncDec == 1:\n",
        "                IntX=1\n",
        "              else:\n",
        "                IntX=-1\n",
        "            else:\n",
        "              IntX=0\n",
        "          else:\n",
        "            IntX=0\n",
        "          return IntX\n",
        "        #/////////////////////////////////////////////////////////////////////////////////////////////////////\n",
        "        def BISC(In):\n",
        "          LenSc= 2 ** XBit\n",
        "\n",
        "          sc=np.zeros(LenSc)\n",
        "\n",
        "          for x in range(XBit):\n",
        "            i=(2 ** x)-1\n",
        "            while i<LenSc:\n",
        "              sc[i] = In[x]\n",
        "              i+= 2 ** (x+1)\n",
        "\n",
        "          sc = np.flip(sc)\n",
        "          return sc\n",
        "\n",
        "        #/////////////////////////////////////////////////////////////////////////////////////////////////////\n",
        "        def DecimalToBinary(num,XBit):\n",
        "          i = 0\n",
        "          bnum = []\n",
        "          Binary=''\n",
        "          while num!=0:\n",
        "            rem = num%2\n",
        "            bnum.insert(i, rem)\n",
        "            i = i+1\n",
        "            num = int(num/2)\n",
        "          i = i-1\n",
        "          while i>=0:\n",
        "            Binary=Binary+str(bnum[i])\n",
        "            i = i-1\n",
        "\n",
        "          while len(Binary)<XBit:\n",
        "            Binary='0'+Binary\n",
        "\n",
        "          return Binary\n",
        "\n",
        "        #///////////////////////////////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "        output=torch.zeros((input_data.shape[0],self.n_outputs))\n",
        "        #print(\"shapeoutputfc1=\",output.shape)\n",
        "\n",
        "        for batch in range(input_data.shape[0]):\n",
        "\n",
        "            for y in range(self.n_outputs):\n",
        "                #print(\"input_data\",input_data)\n",
        "                #print(\"input_data[batch,:]\",input_data[batch,:])\n",
        "                #print(\"self.weights\",self.weights)\n",
        "                #print(\"self.weights[y,:]\",self.weights[y,:])\n",
        "\n",
        "                X=input_data[batch,:]\n",
        "                W=self.weights[y,:]\n",
        "                #///////////////////////////Quantize input & Weight//////////////////////////////\n",
        "                summation= torch.flatten(X + W)\n",
        "                Xquantize,Wquantize,scale,zeropoint=zeropoint_quantize(X,W)\n",
        "                #print(\"Xquantize\",Xquantize)\n",
        "                #print(\"Wquantize\",Wquantize)\n",
        "\n",
        "                Outputsc=torch.zeros(len(Xquantize))\n",
        "                for i in range(len(Xquantize)):\n",
        "                    sum=0\n",
        "                    XBinary=DecimalToBinary(int(Xquantize[i]),XBit)\n",
        "                    SC=BISC(XBinary)\n",
        "                    andisSC=0\n",
        "                    ValW=int(Wquantize[i])\n",
        "                    while ValW>0:\n",
        "                        OutUDCounter=UpDownCounter(ValW,SC[(len(SC)-1)-andisSC],1)\n",
        "                        andisSC+=1\n",
        "                        ValW=ValW-1#DownCounter\n",
        "                        sum=sum+OutUDCounter\n",
        "                    Outputsc[i]=sum*(2**XBit)\n",
        "                #print(\"Outputsc\",Outputsc)\n",
        "                OutDequantize=zeropoint_dequantize(Outputsc,zeropoint,scale,summation)\n",
        "                #print(\"OutDequantize\",OutDequantize)\n",
        "                output[batch,y] = torch.sum(OutDequantize) + self.biases[y]\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "9dRb9aQzfZvv"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FCLayer2(nn.Linear):\n",
        "    def __init__(self, n_inputs, n_outputs):\n",
        "        super(FCLayer2, self).__init__(n_inputs, n_outputs)\n",
        "        print(\"n_inputsfc2=\",n_inputs)\n",
        "        print(\"n_outputsfc2=\",n_outputs)\n",
        "\n",
        "        self.n_inputs = n_inputs\n",
        "        self.n_outputs = n_outputs\n",
        "\n",
        "        # Initialize weights and biases as PyTorch tensors\n",
        "        self.weights = layer_weightsfc2#nn.Parameter(torch.randn(n_outputs, n_inputs))\n",
        "        self.biases = layer_biasfc2#nn.Parameter(torch.zeros(n_outputs))\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        #////////////////////////////Skippy Multiplicant///////////////////////////////////\n",
        "        XBit=8\n",
        "\n",
        "        def UpDownCounter(Enable,x,IncDec):\n",
        "          IntX=int(x)\n",
        "\n",
        "          if Enable != 0:\n",
        "            if IntX == 1:\n",
        "              if IncDec == 1:\n",
        "                IntX=1\n",
        "              else:\n",
        "                IntX=-1\n",
        "            else:\n",
        "              IntX=0\n",
        "          else:\n",
        "            IntX=0\n",
        "          return IntX\n",
        "        #/////////////////////////////////////////////////////////////////////////////////////////////////////\n",
        "        def BISC(In):\n",
        "          LenSc= 2 ** XBit\n",
        "\n",
        "          sc=np.zeros(LenSc)\n",
        "\n",
        "          for x in range(XBit):\n",
        "            i=(2 ** x)-1\n",
        "            while i<LenSc:\n",
        "              sc[i] = In[x]\n",
        "              i+= 2 ** (x+1)\n",
        "\n",
        "          sc = np.flip(sc)\n",
        "          return sc\n",
        "\n",
        "        #/////////////////////////////////////////////////////////////////////////////////////////////////////\n",
        "        def DecimalToBinary(num,XBit):\n",
        "          i = 0\n",
        "          bnum = []\n",
        "          Binary=''\n",
        "          while num!=0:\n",
        "            rem = num%2\n",
        "            bnum.insert(i, rem)\n",
        "            i = i+1\n",
        "            num = int(num/2)\n",
        "          i = i-1\n",
        "          while i>=0:\n",
        "            Binary=Binary+str(bnum[i])\n",
        "            i = i-1\n",
        "\n",
        "          while len(Binary)<XBit:\n",
        "            Binary='0'+Binary\n",
        "\n",
        "          return Binary\n",
        "\n",
        "        #///////////////////////////////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "        output=torch.zeros((input_data.shape[0],self.n_outputs))\n",
        "        #print(\"shapeoutputfc1=\",output.shape)\n",
        "\n",
        "        for batch in range(input_data.shape[0]):\n",
        "\n",
        "            for y in range(self.n_outputs):\n",
        "                #print(\"input_data\",input_data)\n",
        "                #print(\"input_data[batch,:]\",input_data[batch,:])\n",
        "                #print(\"self.weights\",self.weights)\n",
        "                #print(\"self.weights[y,:]\",self.weights[y,:])\n",
        "\n",
        "                X=input_data[batch,:]\n",
        "                W=self.weights[y,:]\n",
        "                #///////////////////////////Quantize input & Weight//////////////////////////////\n",
        "                summation= torch.flatten(X + W)\n",
        "                Xquantize,Wquantize,scale,zeropoint=zeropoint_quantize(X,W)\n",
        "                #print(\"Xquantize\",Xquantize)\n",
        "                #print(\"Wquantize\",Wquantize)\n",
        "\n",
        "                Outputsc=torch.zeros(len(Xquantize))\n",
        "                for i in range(len(Xquantize)):\n",
        "                    sum=0\n",
        "                    XBinary=DecimalToBinary(int(Xquantize[i]),XBit)\n",
        "                    SC=BISC(XBinary)\n",
        "                    andisSC=0\n",
        "                    ValW=int(Wquantize[i])\n",
        "                    while ValW>0:\n",
        "                        OutUDCounter=UpDownCounter(ValW,SC[(len(SC)-1)-andisSC],1)\n",
        "                        andisSC+=1\n",
        "                        ValW=ValW-1#DownCounter\n",
        "                        sum=sum+OutUDCounter\n",
        "                    Outputsc[i]=sum*(2**XBit)\n",
        "                #print(\"Outputsc\",Outputsc)\n",
        "                OutDequantize=zeropoint_dequantize(Outputsc,zeropoint,scale,summation)\n",
        "                #print(\"OutDequantize\",OutDequantize)\n",
        "                output[batch,y] = torch.sum(OutDequantize) + self.biases[y]\n",
        "\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "7Luc-1tGfdLQ"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "Szx7DzUSe28y"
      },
      "outputs": [],
      "source": [
        "class MyModel(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(MyModel, self).__init__()\n",
        "\n",
        "        self.conv1 =  nn.Conv2d(1, 6, kernel_size=5)\n",
        "        self.relu1=nn.ReLU()\n",
        "        self.maxpool1=nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
        "        self.relu2=nn.ReLU()\n",
        "        self.maxpool2=nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
        "        self.conv3 = nn.Conv2d(16, 120, kernel_size=5)\n",
        "\n",
        "        self.fc1 = FCLayer1(120, 84)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.fc2 = FCLayer2(84, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.relu1(out)\n",
        "        out = self.maxpool1(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.relu2(out)\n",
        "        out = self.maxpool2(out)\n",
        "        out = self.conv3(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.fc1(out)\n",
        "        out = self.relu4(out)\n",
        "        out = self.fc2(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "NQhLeRYwe4Lx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2c532f0-8062-4d92-f9cc-350d923a4402"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_inputsfc1= 120\n",
            "n_outputsfc1= 84\n",
            "n_inputsfc2= 84\n",
            "n_outputsfc2= 10\n"
          ]
        }
      ],
      "source": [
        "model1 = MyModel(num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApaM4XD0fi9l",
        "outputId": "785ffcfe-9c00-414d-e481-5d431b1adecc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ],
      "source": [
        "model1.load_state_dict(torch.load('/content/drive/MyDrive/lenet-weight/weightsLenetPytorch3Conv-2.pt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QaVILxwW2iEU",
        "outputId": "136926c2-47e0-49a7-de3d-c15ed14b73b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-115-d432a0e2c37e>:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  tensor_input = torch.tensor(X)\n",
            "<ipython-input-115-d432a0e2c37e>:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  tensor_weight = torch.tensor(W)\n",
            "<ipython-input-115-d432a0e2c37e>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  Mins[0]=torch.tensor(torch.min(tensor_input))\n",
            "<ipython-input-115-d432a0e2c37e>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  Mins[1]=torch.tensor(torch.min(tensor_weight))\n",
            "<ipython-input-115-d432a0e2c37e>:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  Maxs[0]=torch.tensor(torch.max(tensor_input))\n",
            "<ipython-input-115-d432a0e2c37e>:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  Maxs[1]=torch.tensor(torch.max(tensor_weight))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total= 5\n",
            "total= 10\n",
            "total= 15\n",
            "total= 20\n",
            "total= 25\n",
            "total= 30\n",
            "total= 35\n",
            "total= 40\n",
            "total= 45\n",
            "total= 50\n",
            "total= 55\n",
            "total= 60\n",
            "total= 65\n",
            "total= 70\n",
            "total= 75\n",
            "total= 80\n",
            "total= 85\n",
            "total= 90\n",
            "total= 95\n",
            "total= 100\n",
            "total= 105\n",
            "total= 110\n",
            "total= 115\n",
            "total= 120\n",
            "total= 125\n",
            "total= 130\n",
            "total= 135\n",
            "total= 140\n",
            "total= 145\n",
            "total= 150\n",
            "total= 155\n",
            "total= 160\n",
            "total= 165\n",
            "total= 170\n",
            "total= 175\n",
            "total= 180\n",
            "total= 185\n",
            "total= 190\n",
            "total= 195\n",
            "total= 200\n",
            "total= 205\n",
            "total= 210\n",
            "total= 215\n",
            "total= 220\n",
            "total= 225\n",
            "total= 230\n",
            "total= 235\n",
            "total= 240\n",
            "total= 245\n",
            "total= 250\n",
            "total= 255\n",
            "total= 260\n",
            "total= 265\n",
            "total= 270\n",
            "total= 275\n",
            "total= 280\n",
            "total= 285\n",
            "total= 290\n",
            "total= 295\n",
            "total= 300\n",
            "total= 305\n",
            "total= 310\n",
            "total= 315\n",
            "total= 320\n",
            "total= 325\n",
            "total= 330\n",
            "total= 335\n",
            "total= 340\n",
            "total= 345\n",
            "total= 350\n",
            "total= 355\n",
            "total= 360\n",
            "total= 365\n",
            "total= 370\n",
            "total= 375\n",
            "total= 380\n",
            "total= 385\n",
            "total= 390\n",
            "total= 395\n",
            "total= 400\n",
            "total= 405\n",
            "total= 410\n",
            "total= 415\n",
            "total= 420\n",
            "total= 425\n",
            "total= 430\n",
            "total= 435\n",
            "total= 440\n",
            "total= 445\n",
            "total= 450\n",
            "total= 455\n",
            "total= 460\n",
            "total= 465\n",
            "total= 470\n",
            "total= 475\n",
            "total= 480\n",
            "total= 485\n",
            "total= 490\n",
            "total= 495\n",
            "total= 500\n",
            "total= 505\n",
            "total= 510\n",
            "total= 515\n",
            "total= 520\n",
            "total= 525\n",
            "total= 530\n",
            "total= 535\n",
            "total= 540\n",
            "total= 545\n",
            "total= 550\n",
            "total= 555\n",
            "total= 560\n",
            "total= 565\n",
            "total= 570\n",
            "total= 575\n",
            "total= 580\n",
            "total= 585\n",
            "total= 590\n",
            "total= 595\n",
            "total= 600\n",
            "total= 605\n",
            "total= 610\n",
            "total= 615\n",
            "total= 620\n",
            "total= 625\n",
            "total= 630\n",
            "total= 635\n",
            "total= 640\n",
            "total= 645\n",
            "total= 650\n",
            "total= 655\n",
            "total= 660\n",
            "total= 665\n",
            "total= 670\n",
            "total= 675\n",
            "total= 680\n",
            "total= 685\n",
            "total= 690\n",
            "total= 695\n",
            "total= 700\n",
            "total= 705\n",
            "total= 710\n",
            "total= 715\n",
            "total= 720\n",
            "total= 725\n",
            "total= 730\n",
            "total= 735\n",
            "total= 740\n",
            "total= 745\n",
            "total= 750\n",
            "total= 755\n",
            "total= 760\n",
            "total= 765\n",
            "total= 770\n",
            "total= 775\n",
            "total= 780\n",
            "total= 785\n",
            "Accuracy of the network on the 10000 test images: 20.764331210191084 %\n"
          ]
        }
      ],
      "source": [
        "# Test the model\n",
        "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
        "\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images[0:5]\n",
        "        labels = labels[0:5]\n",
        "        outputs = model1(images)\n",
        "        #input(\"pls enter a code\")\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        print(\"total=\",total)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))"
      ]
    }
  ]
}